YouTube Video ID: pebgrFQ-C7M
Language: en
With Timestamps: False
Generated on: 1753581727.6070824
--------------------------------------------------

You know, a few days ago I did an entire
piece on setting up digital twins. It
was very large. I think I wrote a
hundred some page guide for it. The
point is people took it and said, "This
looks really hard. You wrote all these
pages for it. Help me figure out how to
do it simply. I don't have an enterprise
setup. I'm not making robots learn to
walk in a warehouse. This is not a
Fortune 500 company. I want to use
digital twins without having to write
code." And I took that as a personal
challenge. And so what we are going to
work through today is an actual prompt
to set up your own digital twin
simulation. We have a couple of goals as
we work through this prompt. We want to
understand first how the prompt
functions. It's another one of those
system level prompts. So it actually
walks through an entire process or flow.
It creates a scenario and then it walks
you directly through it all in one
prompt. And we'll see how it works. The
second thing I want to do is I want to
show you some of how it works with
actual real conversations that I had
using that prompt and we'll walk through
them. You'll get both a salary
negotiation conversation because
sometimes they want to game those in
advance and you'll also get a classic
like product approval conversation to
give you a sense of office politics and
how it simulates office politics. It's
interesting to see how different models
affect this simulation. And for just one
more wrinkle, I'm going to give you a
sense of how chat GPT40
compares to chat GPT 03 handling this
prompt and running the simulation. This
is actually one of the clearest examples
I've seen of the practical realworld
differences between those two models. So
with that, let's get to the prompt.
Okay, here we have the digital twin
stakeholder prompt that I constructed. I
helpfully named it V2 because it took
multiple iterations to get this prompt
working and I want to walk through how
it functions. As before, I am running
this in the web browser comet with the
handy assistant pulled up. I had a few
questions when I did this last time. So,
just so you know, this is the Perplexity
web browser and that's an AI assistant
that Perplexity launches with the
browser. I find it useful. I did a write
up on it. Uh, and I find it really
helpful in situations like this where
I'm trying to understand what is going
on with a complicated page. So, as you
can see, the prompt as a whole is quite
large. I will just scroll to the end of
the prompt and you'll sort of start to
see, wow, there's a lot here. So, let's
get into it piece by piece. First, we
set the role. This is not surprising. I
think I've talked before about the idea
that you're setting the role as a way of
invoking a particular semantic space.
And next, you define the mission. In
this case, we have a four-part mission.
This is part of why you'll see better
performance out of 03 than 40 when we
get later into this video. Number one,
get the information out of me to figure
out what what is needed for a realistic
multistakeholder negotiation in my
situation because this is not a specific
prompt for compensation negotiation. It
is a super prompt that actually helps
you to set up whatever simulation you
want to run. Second, ask only one
question at a time. I find it so
overwhelming when LLM's ask too many
questions. So I I tend to include that.
Third, confirm the answer. And then
fourth, when the answers are gathered,
and now we get into the heart of it. You
need to generate a runnable simulation
prompt that embeds every confirmed
detail, contains inline data tables if
the user has not provided external
files, but you can, and includes clear
output rules, and a begin delimter. And
then five,
write the prompt out as plain text and
immediately transition into simulation
mode. One of this was actually one of
the reasons that I went to V2 is because
I found when I was playing with this
prompt and working with it, it is
difficult to get the AI to reliably go
from I'm learning about you and
gathering information to okay, we've got
the information. Now, it's time to
actually run the simulation and be a
digital twin. In this case, multiple
digital twins. So far so good. And
what's interesting is when you get to
the the end of the prompt, it
immediately invokes
it immediately invokes the twins. And so
we'll get down here and see what this
means. But I want you to see the
connection between five where we talk
about what we want the twins to be and
the end here where we talk about each
twin's opening statement. And that's the
way you begin. because this ties this
ties the tokens back when the LLM is
reading this prompt so that it knows
okay we have to remain in character
opening statements of every twin I see
that back here it's like we're doing
this memory management across a larger
prompt and this is a great example of
how that can work okay now the LLM at
this point would still not have all that
it needs to run so let's get to the
question script because we need
something that gets information out of
you one at a time. The obvious first
one, what situation are we negotiating?
List the participants. None of these are
super surprising. List the success
metrics or how you characterize a win.
What twin will you play? And by the way,
this works even if you don't want to
play a twin. So, if you just want to see
the LLM game it out and read the script,
you can say, "I won't play any of the
twins. You're playing them all." And it
will. How many turns before timeout? So,
this is framed as a multi-turn
conversation. And so you can say three,
you can say six, you can say whatever
number you want. Please provide key
numbers or attach a file for deals or
comp. Otherwise, give ballpark figures.
If you're salary negotiating, it's
obviously comp. But when I was doing it
for my uh product sample that I'll show
you in a moment, I ignored the comp part
and it said deal. So I put in like deal
data and like where our pipeline was at
and all of that. These are all madeup
numbers. Um, and I felt really free to
add the data I wanted. I will also call
out that this is a somewhat flexible
user response prompt. It is designed to
be a place where you can put a lot of
information and you'll see in one of the
sample chats how I'm able to pack in
like 300 words of information which is
really more than I designed the prompt
to take and the prompt is able to handle
that. Okay, constraints and policies
that's always important in any
negotiation and then your output
preferences, transcript style, word
limit, etc. So at that point uh I
actually define a confirmation phrase.
Now why do I define a confirmation
phrase? Why do I say after the user
answers each question, this is what you
should say? Well, number one, I define a
confirmation phrase because it is
critical to actually understand what the
user said to get this world building
right. With a lot of prompts, if you
write the prompt, you can do a second
turn and kind of figure it out if you're
trying to refine. But with a world
building or digital twin kind of prompt,
the world has to be right from the
get-go. And so I need to make sure that
it understands, right? And so I have it
come back and actively listen and
summarize on purpose. Okay, so it's
actively listened. It's collected all
the information. We now get to the
runnable prompt template. When all the
answers are confirmed, please fill the
placeholders and output you are the
digital twin negotiation arena host. So,
in a sense, you might be wondering, why
the heck is this here? Purpose, mode,
instructions, reference. Haven't we been
talking long enough? Why does this go
on, Nate? Well, I'll tell you why it
goes on. It goes on because we are
trying to bake in some degree of
consistency in this prompt and giving it
a purpose, a mode, an effort level,
which by the way, you can set
differently. Like I set it at high, but
if you want to change this prop before
running and set it at low, you can. Uh
you can also set the scenario and
hardcode it here if you want or it will
fill in the scenario for you based on
your answers. It will then output all
the rest of this based on your answers
and the reference. And as it does this,
it is literally encoding into the stream
of conversation everything it needs to
know to keep it going. This little piece
here does the important job of acting as
a conversational anchor. It acts as a
conversational anchor so that the rest
of the world building will work. Right?
If you have ever tried to do digital
twin stuff, you know that the
personality is drifting can be a
problem. This is part of how we control
for that. Having this clear reiteration
before we start. Finally, we begin
insert each twins opening statement
right after the delimiter. So that's the
prompt. That is what we will run. That
is what I will show you. Before we go
further, I want to actually go down. I
actually broke this out into principles
that we can look at. Uh, and I also
broke out takeaways. And so I want to
spend a second
looking at the takeaways and the
principles and make sure you understand
why I did what I did before I go to the
actual examples. So principle number
one, identity lockin. I want to make
sure that it's a digital twin and I want
to make sure I invoke that corner of
latent space. It is a deterministic
state machine which is a fancy way of
saying I am deliberately creating a
fixed numbered question script to gather
input on a path. This turns an
open-ended chat into something that is
very repeatable and something where the
uh process can be deliberately shifted
for ease of use in learning about
different future timelines. So, if you
want to game out a timeline where you
open with 210 for your comp or a
timeline where you open with 180 or a
timeline where you open with 150, you
can do all of those things in separate
chats and you get a really tight
controlled uh simulation of how that
conversation might unfold. Similarly, if
you're doing a job interview, which you
could use for this, you can game out in
multiple different chats, what if I
answered this way? and you can watch
your other stakeholders respond and
actually like game that through and
think that through more easily. Humans
are not great at simulating entire
digital scenarios in our head with
multiple stakeholders at high fidelity.
That is what this prompt is designed to
do. Progressive disclosure. We talked
about asking one question at a time.
Echo confirmation loop. Rephrase.
Actively listen. Actively listen might
have been a better way to put this.
Explicit output contract. This is the
contract that we described up here that
sets the terms of the debate. It says
here's your purpose, here's your mode,
here's your effort, here's your
instructions, here's your references,
and here's your output. It's really
important to be clear and explicit about
that when you're doing digital twin
work. Okay. And then we get to handoff
embed all the requisite data inside the
runnable block. This was one of the
challenges I had and this is why we
named this V2. It is hard to get all the
data inside a runnable block, collect
all of it, make it run inside the same
prompt. One of the keys to getting it to
do that is to be very very explicit
about that reiteration of the contract
and explicit in the questions about what
you are gathering. And so if you look
back here, we are being very explicit
about what we want in this question
setup here, these nine questions.
And we are specifying it needs to be
runnable and we are specifying it needs
to begin now. We are not giving the
model any choice. We are saying here's
all the information and you must begin
now. And that was very deliberate.
Context switching invokes the model's
opening moves very deliberately and we
define what we expect for the first move
which helps the model get into that
space. We say right up here, we need you
to make opening statements that because
you could begin a lot of different ways,
but we say opening statements because
that enables the model to enter the
simulation space in a predictable way.
If you leave that blank, you are giving
the model cart blanch to open any way it
wants. And you don't want that for a
predictable simulation builder. You
actually want a little bit of
predictability so you can start to
insert variables and learn. Okay. From
there we get into uh visual parsing and
bounded variables. So users can set
rounds, they can set word limits, users
can um actually see what is going on
easily because we're using those asy
gutters like the delimiters. I don't
want to say these are fantastic
principles. Like don't make me tell you
that begin now with special asy gutters
is somehow going to be more magical than
begin now. It's not. But it sure is
easier to read. And if we're reading
these large prompts like I've been
describing, it is sometimes nice to have
clean gutters and bulleted lists.
Finally, there are error rules for
failure modes. And part of the error
rules I've already called out to you.
Like if you go back up here, one of the
hidden error uh rules is the
confirmation phrase. If there is a
problem, you are going to see it because
it's going to come back and tell you
because the user answer paraphrase is
going to be correct. But there's other
ones too like do not ask further setup
questions after the honorable prompt is
admitted. We are scoping down so it
doesn't go forever. We are demanding
that it remain in character. We want to
ensure it retains the detail. We are
demanding that it only asks one question
at a time. We are giving it a lot of
constraints. Okay. Now I want to finish
by talking about the structure and
starting with the system ro declaration
is classic. Getting to the mission is
the correct overall next step. And by
the way, these are larger pro uh prompt
structures that you can use in other
prompts, not just for these super macro
prompts, right? If you start with, hey,
here's my system role to move you into
latent space. Here's the mission that I
have for you. Uh and then here's the
content that I want you to engage with.
In this case, it's the script. you're
going to be in a good spot for a lot of
different prompts. If you frame how you
want the response to work, which is the
confirmation phrase template in my case,
but could be something totally different
for a prompt of yours, it's going to be
really, really helpful. And finally, if
you specify a contract and how you
begin, if you're trying to do a super
prompt like this and you specify a
runnable prompt contract at the end and
you also specify how you want the model
to begin, it increases the odds that
your prompt is going to actually run
successfully. Okay, we are going to skip
the micro details that add delight.
We've talked about the asy. We've talked
about markdown. Aren't you glad I can
make things pretty? Uh we will talk
briefly about how these pieces reinforce
one another. The fixed question order
reinforces the deterministic state
machine. The builder never deviates from
the script and always gets the questions
the same way. And you can treat it like
an automaton in that regard. The only
difference is the power of the LLM
behind it, which is really interesting
because you'll see in the examples how
03 versus 40 is different. If you are
teaching this, and I know some folks
that watch my videos teach my work to
others, and that's fantastic. You want
to remind people of the principle of
progressive disclosure and the
importance of template integrity. Be
careful with your placeholders. Be
careful with asking too much of the
model in one go without those checks and
balances that I showed you in the
prompt, without asking it to reiterate
the contract, without asking it to be
specific in its summary as it comes back
to you. We are giving the prompt the
token scaffolding it needs to be
successful. Finally, potential pitfalls.
You want to make sure that you are
giving it limits. And so that is one of
the things that we did not go as
explicit here that I find that the model
tends to get to in practice, as we'll
see. You want to not be
overcommitting the model to responses it
can't deliver. Another example that
would have strengthened this prompt more
that you might want to use if you add
more files is to summarize key numbers
in two to three lines to force the model
to answer back on files because you
don't want it to just skip over the file
content. So there are little things that
think of these is essentially tweaks you
can make depending on what you're
looking to accomplish. We've gone on
long enough. Let's move to digital twin
negotiation builder v2 using 03 and see
how the conversation went. Here we are.
I run the prompt just as you described.
The situation we're negotiating is a
product pitch LLM to SQL project. It
understands it. It comes back. Here's
who I'm pitching to. CEO, CTO, CFO,
director, me and the CRO
here. I I want approval to build the
product and launch. That's my goal. I
have been in these situations. I'm
picking something I know well so I can
assess the quality. Next question. Which
twin will you play? I'm going to play me
and be the director of product. Great.
How many turns before timeout? Let's do
three rounds cuz most CEOs don't
tolerate more than that. And then I
throw it a curveball. This was not
planned for in the prompt. I throw it
300 words on my product and I say this
is what I pitched them. Right? It's all
about how amazing this imaginary product
is. I I am okay with the numbers being
made up because this is an example and
this is my ask Q3 roll out request. Got
it. Next question. Provide any key
numbers and I basically say I could have
said just stick with these numbers up
here but I want to think about the deals
in this case because I think that's
going to matter for the simulation. Most
most seuite conversations in a B2B SAS
have to do with deals at some level. I
say we're chasing million-dollar deals
with enterprise clients. We've had a
pipeline slowdown in the last 6 months.
Okay, great. The key data we're pursuing
is this. It reiterates. It asks me to
talk about the different characters. And
so this is me like you basically make up
the characters you want. If you don't
know, you have to infer and guess. I had
fun making these up. These are
fictional, but they're plausible, right?
The CTO is worried about hitting the
schedule. The director of engineering is
somewhat hostile because he thinks we're
we're missing on core product. The CFO,
she's worried about close rates and
pipeline. The CEO, he's a business guy.
He believes in enterprise and he loves
AI, but he doesn't know how it works.
Like these are all very plausible and I
need a promotion, right? Like because I
I' I've definitely been in that spot. So
there it is. And then it asks, "What are
the open legal or approval issues?"
Super simple. Uh there's an open legal
issue around LLM and data privacy.
There's also an open question around
pricing for usage, which is a huge issue
more broadly. Different video. It
reiterates and I say, "Please generate a
script style transcript. Set the limit
for debrief at 500 words. Leave room to
speak." Now it is going to reiterate the
contract. This is a contract. It writes,
it's all writing all of this and it
begins with opening statements. Begin
out round one now. And it does. So the
CEO sets the terms. The CTO says in line
with his character, it's viable, but I
worry. Enterprise pro prospects ask. You
get the idea. It's now my move. I
respond. I say, listen, we have to do
this because we are losing in the
market. A strong AI feature can add
sizzle, and if we combine it with uh the
CRO's concerns around SOCK 2, we should
unstick some deals. It then moves
forward. I'm not going to read all of
these in detail, don't worry. We get to
the second round, we see that it's
playing the roles correctly. I then
start to negotiate. I say, "Listen, we
shouldn't just look at new customers. We
should look at existing expansion
revenue. I'm starting to move forward.
This is third round. I've got to finish
up." The CRO basically says, "Okay, if
we're talking about expansion revenue,
maybe I can get on board." The CFO who's
been worried about the money says,
"Okay, that that unlocks that for me,
etc." And at the end, it gives me a
scorecard. It says, "I got a partial
approval." It gives itself a scorecard
for twin realism and it gives me a
debrief. What did it hinge on? Where did
the tension surface? How did it work? I
think this is the most useful part of
this whole exercise because if you want
to game out different scenarios like for
a job interview, you get debriefs on
your performance like this. I think it's
really cool. Next, let's actually go
into the job search arena and let's look
at a real example of compensation
negotiation, the final stage of job
search. Okay, we are back. I'm not going
to rerun this prompt in more detail. You
get the idea. We went through the
different situations. We're negotiating
compensation this time. Smaller group,
head of HR, CPO, and me. Uh, the win is
dollars. We want more of them. Uh, we
have six rounds. And I give it uh my
current offer. Uh, I'm totally making up
these numbers, but they're not
completely implausible.
And it says it's got it. I give it
personality. The CPO is a hard-bitten
guy. HR wants to follow policy. I
realize this sounds right out of central
casting, but we're having fun. Okay. And
I give it instructions. Again, all of
this is necessary to set up the fidelity
of the digital twins world. And so it
then starts to print out what it's going
to do. It prints out all of the stuff
it's working with, all the stuff it
knows so far. Begins round one. It gives
opening statements based on what I've
said, what the CPO said, what HR has
said, and then it asks me to go from
here. Essentially, they say no. And so I
say, "Hey, this reflects the market
average. Like, this is legit." And so
they start to shift a little bit. They
bring up finance and approvals as an
issue. Uh HR is really concerned about
it. It's my move. So I say, "Okay, I
want to play with equity and bonus a
little bit. What if I drop the comp
down?" This is actually how a lot of
comp negotiations go. But we don't get
the chance to play them out. The exact
reason we have digital twins is so that
we can do stuff like this in a
controlled environment because I could
simulate this whole thing again at a
different number and walk out with a
different sense of how the narrative
went. Okay, so director of product, CPO
and head of HR come back, they talk
through that we're getting closer. I
love that it's keeping a concession
ledger so we can actually see how it's
going. I wish that was true in real
life. This is one way that it's easier
in this uh in this simulated
environment.
uh we are arguing over the clause for
equity. I can live with a clause for
more equity that's dependent on period
of employment, I say, but I don't think
it's reasonable to make it dependent on
product revenue goals if the CPO's
equity is not similarly tied. And the
CPO dodges the question as real CPOS
tend to do and then moves on and says
fine, we won't we won't make that a
thing. We are now close to an agreement
as you can see from here.
And at the end of the day, the only
thing that's an issue is what if finance
pushes back? And I provide a savvy
approach in terms of how finance pushes
back. What do we do? How do we hand? And
so then they come back and they say,
"Okay, basically we've got a deal." I
say, "Let's get a deal done." It then
gives me a scorecard. It gives me comp.
It gives me uh a debrief on how it went
and it gives me like madeup next steps
for what I would do. I think this is the
most useful bit. It talks through how I
handled finance resist. It talks through
like the good the way I anchored on
local market data and insisted on par
with executive incentives was really
smart. Um it's just going to give you
feedback and you can you can ask it at
that point will give me more critical
feedback, right? Like push me harder and
it will do that. Last thing I want to
show you is what happens if we run this
exact same scenario token for token.
Same like same answers all the way
through but we used a different model.
So, we're going to go and use Chad
GPT40.
Okay, here we are. Chad GPT40 up here. I
am not going to rerun. These are all
exactly the same. Exactly the same. Set
it up exactly the same. Yep. Great. Go.
It then prints out
the entire contract and begins. What I
notice immediately with 40 is that 40
has more personality. Look at the way
the CPO talks.
>> I've been in this game a long time, and
I'll be straight with you. We think
you're the right person, but we're not
playing paying fantasy baseball
salaries. That is 40 language. That is
not 03 would never do that. So, the
personality difference pops out right
away. I come back as much as I can. I'm
using token for token the same thing.
So, we focus you on model differences.
Um, the CPO continues to have
personality. Head of HR continues to be
very bland. And moving forward, I say,
okay, I can, you know, negotiate. I can
adjust. Again, I'm keeping this exactly
the same as I did for the 03 example.
The CPO then begins to praise me, which
would not happen in real life and is one
of our first indications that the model
is not as smart as 03. This is a little
bit out of character for a CPO, but he
continues to be very, very chatty. I
then give the same response, and what's
interesting is I decided to give the
same response to keep the model as clean
as possible. But you'll notice that I
gave that response when we were talking
about clauses and 4 is not talking about
clauses here. So this is me literally
throwing a little bit of a curveball to
keep the exact same token stream for the
model to maintain the test as cleanly as
possible. Okay, CPO comes back. Head of
HR understands. And you know what's
interesting? This is where the model
really begins to diverge right here.
They just agree. That did not happen
with 03. One of my top takeaways looking
at this example is that 4 shows its
dumbness by being too agreeable in
digital twin scenarios. If you were
simulating this with 4, I worry you
would walk away with a false idea of how
tough these negotiations can be. I
thought 03 did a much better job
simulating how deep into the weed
sometimes comp negotiations can go. And
here I I just say I can get a deal done,
right? like they've basically given me
what I want. And if you walk out, you
can see that I got more cash. The offer
was at 190 and I walked out at 200
instead of 194. Essentially, you can
literally measure in dollars the
dumbness of the model. 40 gave me $6,000
that 03 was able to withhold for me
because it was a sharper negotiator,
which is actually better for you when
you're looking to simulate. So, with
that, there you go. We have three
different actual conversation examples.
We have a detailed breakdown of how to
build a digital twin in chat GPT. You
don't have to do anything. You don't
have to run any code. You understand the
principles behind that prompt. You
understand how it hangs together. You
understand how you could tweak it or
adjust it for other scenarios. And the
prompt itself is a super prompt, which
means that you can run that prompt and
give it other scenarios. Don't just do
compensation. Don't just do product
proposals. You can do it with sales
negotiations. You can do it with job
interviews. Anything that requires
multiple parties to discuss and agree,
you got options. You can run this prompt
to simulate it. This is what I mean
about the power of AI for digital twins.
It is a huge deal. Most of us are
sleeping on it. And I think part of it
is it's been tricky to know how to
prompt. And so this is my attempt to
like bring the bridge over so that you
can see like we're going to build the
bridge. You can see how the prompt works
and it's not a mystery anymore. Hope
that was helpful. Tips.